\documentclass[11pt]{report}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{ragged2e}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{pgf,tikz}
\usepackage[shortlabels]{enumitem}
\usepackage{color}
\usepackage{pgfplots}
\usepackage[margin = 1 in]{geometry}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage{multicol}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{listings}
\renewcommand{\footrulewidth}{0.4pt}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{defn}{Definition}[chapter]
\newtheorem{lemma}{Lemma}[chapter]

\theoremstyle{definition}
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{example}{Example}[chapter]

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\user}{}
\newcommand{\xlr}[2]{#1 \left(#2\right)}
\newcommand{\clr}[2]{#1 \left\{ #2 \right\}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\mat}[1]{\mathbf{#1}}
\lhead{ECE 530 - Fall 2023 at University of Illinois at Urbana-Champaign}
\rhead{HW4}
\lfoot{Author: \textcolor{red}{Eric Silk, esilk2}}
\rfoot{Due: Wednesday, Oct 4}
\begin{document}


\section*{Problem 1: Proving what Newton method is up to}
\subsection*{Problem Statement}
Newton's method minimizes the local quadratic approximation of a function, if
its Hessian at the current iterate is positive definite. We showed that Newton's
method indeed finds a \textit{local} minimizer of the quadratic approximation.
Here, we show that it finds the \textit{global} minimizer through the following
steps.

\subsubsection*{a}
If $Q$ is any PD matrix, show that:
\[ \frac{1}{2}x^TQx+c^Tx =  \frac{1}{2}(x+Q^{-1}c)^TQ(x+Q^{-1}c)-\frac{1}{2}c^TQ^{-1}c \]

\subsubsection*{b}
Using the prior result, argue that the function $\frac{1}{2}x^TQx+c^Tx$ is
minimized \textit{globally} at $x^*=-Q^{-1}c$.

\subsubsection*{c}
Recall that for any function $f:\mathbb{R}\rightarrow\mathbb{R}$, its local quadratic
approximation at $x^k$ is given by:
\[
	f^{q}(x) \coloneqq
	f(x^k)+[\nabla f(x^k)]^T(x-x^k)+\frac{1}{2}(x-x^k)^TH(x^k)(x-x^k)
\]
Assume the Hessian $H(x^k)$ is positive definite. Utilize the prior result to
conclude that the \textit{global} minimizer of $f^q$ is given by
$x^k-[H(x^k)]^{-1}\nabla f(x^k)$. Notice that the minizer is indeed $x^{k+1}$,
as defined by Newton's method.


\subsection*{Solution}
\subsubsection*{a}
Simply expand the right side:
\[
	=\frac{1}{2}(x^TQ(x+Q^{-1}c)+c^T(Q^{-1})^TQ(x+Q^{-1}c))-\frac{1}{2}c^TQ^{-1}c
\]
\[
	=\frac{1}{2}(x^TQx+x^TQQ^{-1}c+c^T(Q^{-1})^TQx+c^T(Q^{-1})^TQQ^{-1}c)-\frac{1}{2}c^TQ^{-1}c
\]
Noting that (per the instructor comment on Piazza) $Q^T=Q$ and that $Q^{-1}\succ0$:
\[ =\frac{1}{2}(x^TQx+x^Tc+c^Tx+c^TQ^{-1}c)-\frac{1}{2}c^TQ^{-1}c \]
Finally, note that $c^Tx = x^Tc \implies c^Tx+x^Tc=2c^Tx$ and thus:
\[
	=\frac{1}{2}x^TQx+c^Tx+\frac{1}{2}c^TQ^{-1}c-\frac{1}{2}c^TQ^{-1}c
	=\frac{1}{2}x^TQx+c^Tx
\]
\qed
\subsubsection*{b}
By definition, if $Q\succ0$, $x^TQx>0\forall x\neq 0$ and $x^TQx=0\iff x=0$.
As such, $\min_x x^TQx = 0$ and $\argmin_x x^TQx = 0$.
Given the prior result, if we substitute in $x\coloneqq x^*=-Q^{-1}c$ we find:
\[
	\frac{1}{2}x^TQx+c^Tx
	= \frac{1}{2}(x+Q^{-1}c)^TQ(x+Q^{-1}c)-\frac{1}{2}c^TQ^{-1}c
\]
\[
	= \frac{1}{2}(-Q^{-1}c+Q^{-1}c)^TQ(-Q^{-1}c+Q^{-1}c)-\frac{1}{2}c^TQ^{-1}c
\]
\[
	= \frac{1}{2}(0)^TQ(0)-\frac{1}{2}c^TQ^{-1}c
	= -\frac{1}{2}c^TQ^{-1}c
\]
which is a constant. Note that the term that was dropped cannot be negative. Thus, this
result must be the global minimum of the function.
\qed
\subsubsection*{c}
First, we need to note that $f(x^k)$ is a constant, as $x^k$ is a parameter
and will not change as a function of $x$. Furthermore:
\[\argmin_{x} g(x) = \argmin_x g(x) + k\]
i.e. addition of a constant does not change the location of the optimum.

So: simply let
\[ c'\coloneqq \nabla f,\ x' = x-x^k, Q' = H \]
We can then see it matches the form:
\[f(x^k) + \frac{1}{2}x'^TQ'x'+c'^Tx'\]
Given our expression for $x'$, we can see
\[
	x'^* = x^*-x^k = x^k-H^{-1}\nabla f - x^k
	= -H^{-1}\nabla f
	= -Q'^{-1}c'
\]
which is identical to the result in the prior section.\footnote{
	Kudos to Will V. for pointing out the expression should be massaged to match the
	prior expression from the get-go. I had previously spent a few hours chasing my
	tail trying to make the expressions match \textit{after} I substituted the value
	for the optimum. This was fruitless.
}\qed



\newpage
\section*{Problem 2: Newton's method needs a touch-up}
\subsection*{Problem Statement}
In Newton's method, if the Hessian at the current iterate $H(x^k)$ is not PD,
then $-[H(x^k)]^{-1}\nabla f(x^k)$ may not be a descent directon. Then, we modify the Hessian to
$H(x^k)+D^k$ where $D^k$ is a diagonal matrix with nonnegative diagonal entries. Let us design
$D^k$ to ensure that $H(x^k)+D^k$ is PD. The following corollary of Gershgorin's circle theorem
will prove useful.

\begin{theorem}
	If $\lambda$ is any eigenvalue of an arbitrary matrix $A\in\mathbb{R}^{n\times n}$, then
	\[|\lambda-A{ii}|\leq\sum_{j\neq i}|A_{ij}|\]
	for some $i=1,\ldots,n$.
\end{theorem}
\subsubsection*{a}
Using this theorem, find a sufficient condition on the diagonal entries of $A$
s.t. all eigenvalues of $A$ are positive.
\subsubsection*{b}
Using the prior result, find a diagonal matrix $D^k$ s.t. that all eigenvalues of $H(x^k)+D^k$
are nonnegative, and all diagonal entries of $D^k$ are nonnegative.
\subsubsection*{c}
If any eigenvalue of $H(k)+D^k$ is close to zero but positive, then it is close
to being singular and its inverse is susceptible to noise. Modify your answer in the prior section to
ensure that all the eigenvalues are greater than $\frac{1}{2}$.
\subsubsection*{d}
The file applyNewtonMethod.m implements a basic newton method to the function
\[f(x_1, x_2)\coloneqq \cos(x_1^2-2x_2)+\sin(x_1^2+x_2^2)\]
starting from $x^0=(1.2,0.5)$. The program also draws the contour plot and the surface plot of $f$.
\begin{enumerate}
	\item Verify the Hessian at the starting point is NOT PD.
	\item Does the Newton method converge? If yes, does it converge to a local
	      minimizer of $f$?
	\item Fill in the missing code in modifyHessian.m that takes $H(x^k)$ as
	      input and gives $H(x^k)+D^k$ as output. Utilize your condition in part
	      (c) to design $D^k$. Submit your code. Using your code, compute
	      $H(x^0)+D^0$; i.e the modified Hessian at the starting point.
	\item Uncomment the relevant lines in applyNewtonMethod.m to run the
	      modified Newton method. Report if the algorithm converges to a local
	      minimizer of $f$.
\end{enumerate}

\subsection*{Solution}
\subsubsection*{a}
Using the geometric interpretation (borrowing heavily from the Wikipedia page\footnote{
	\href{https://en.wikipedia.org/wiki/Gershgorin_circle_theorem\#Example}
	{Wikipedia: Gershgorin's Circle Theorem Example (click me!)}
}), we can note that the theorem essentially describes a bounded estimate of an
eigenvalue, where the center of circle is located at the value of the diagonal
element in a row ($A_{ii}$) and its radius is the sum of the magnitude of all
other elements in the row ($\sum_{i\neq j}A_{ij}$). In order to bound the
eigenvalue to be positive, this circle must exist solely within the RHS of the
complex plane. Thus: Given a square matrix $A\in\mathbb{R}^{n\times n}$, a
sufficient (albeit not necessary!) condition is:
\[ A_{ii} > \sum_{i\neq j}|A_{ij}| \forall i \in 1,\ldots,n \]
In plain english: we have to move the bounding circle via a translation of its
center (the diagonal element of that row) to be larger than than he radius.  If
we only require non-negative eigenvalues, the relationship could be relaxed to
be ``greater than or equal to''.

\subsubsection*{b}
\[
	D_{ii} + A_{ii} > \sum_{i\neq j}|A_{ij}|
	\implies D_{ii} > \sum_{i\neq j}|A_{ij}| - A_{ii}
\]

\subsubsection*{c}
\[
	D_{ii} > \sum_{i\neq j}|A_{ij}| - A_{ii} + \frac{1}{2}
\]
\subsubsection*{d}
\textbf{Note}: I used GNU Octave\footnote{\href{https://octave.org/}{GNU Octave (click me!)}}
instead of MATLAB because I'm too lazy to get it installed and working on my machine (plus
I'm not a fan of closed-source proprietary languages).
\begin{enumerate}
	\item I checked \lstinline{all(eig(Hk)>0)} for iterate 0, which reports
	      \lstinline{false}. The matrix is not PD.
	\item It does after 4 iterations. It is not a local minimum; again, the
	      Hessian is evaluated to be not PSD. The necessary second-order optimality
	      condition does not hold: it is not a local minimum.
	\item See code listing at end of the document.
	      \[
		      \tilde{H}^0 = \begin{bmatrix}
			      5.3041 & 4.8041 \\
			      4.8041 & 5.3041
		      \end{bmatrix}
	      \]
	\item It does. Script output at final iteration:
	      \begin{lstlisting}
Iteration #17
Current values of (x,y) = [27.4427     -28.5679]
Current norm of gradient =1.2935e-07
Function value at last iterate =-2
Iterate is a local minimum:True
\end{lstlisting}
\end{enumerate}


%------------------------------------------------------------------------------------------------------------

%------------------------------------------------------------------------------------------------------------

\end{document}
